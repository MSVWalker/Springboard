{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df52d4c",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Data Preparation](#data-preparation)\n",
    "    - [Import necessary libraries](#import-necessary-libraries)\n",
    "    - [Patient_001 Example - Loading wristband + CGM data frames](#patient_001-example-loading-wristband-CGM-data-frames)\n",
    "        - [Inspect DFs and column names](#Inspect-DFs-and-column-names)\n",
    "        - [Column renaming for consistency, strip unnecessary characters, convert to datetime](#column-renaming-for-consistency-strip-unnecessary-characters-convert-to-datetime)\n",
    "    - [**Data sampling mismatch**](#data-sampling-mismatch)\n",
    "        - [Early feature engineering on respective sampling periods](#early feature engineering on respective sampling periods)\n",
    "        - [Acceleration magnitude](#acceleration-magnitude)\n",
    "        - [Peak Detection in electrodermal activity (EDA)](#early-feature-engineering)\n",
    "        - [5 minute aggregate features - mean, std, min, max, 25% quantile, 75% quantile, and skewness](#early-feature-engineering)\n",
    "        - [Merge each dataframe resampling on our glucose measurements (5min)](#merge seperate dfs into 1 resampling on Dexcom (glucose))\n",
    "        - [Drop NaN values](#Drop NaN values)\n",
    "        - [Inspect merged_df](#inspect-merged_df)\n",
    "    - [Universal code to wrangle the data (similar as we did for Patient_001) for each of the 16 patients, creating one large df with each patient's data](#all-patient-merge-code)\n",
    "        - [Same wrangling steps as before](#all-patient-merge-code)\n",
    "        - [Add column with Patient ID from 1 to 16](#all-patient-merge-code)\n",
    "    - [Demographics CSV](#Demographics-CSV)\n",
    "        - [Includes gender, HbA1c, and patient ID](#Demographics-CSV)\n",
    "        - [Load and create pandas df from demographics_csv](#Demographics-CSV)\n",
    "        - [Merge previous df with demographics_df](#demographics-merge)\n",
    "        - [Organize columns in a more intuitive order](#Reorder-columns)\n",
    "        - [Save df with wearables+demographic data to patient_df.csv](#patient-df-csv)\n"
   ],
   "id": "71b5bcd236a84e0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "Prediabetes affects one in three people and has a 10% annual conversion rate to type 2 diabetes without lifestyle or medical interventions. Management of glycemic health is essential to prevent progression to type 2 diabetes. However, there is currently no commercially-available and noninvasive method for monitoring glycemic health to aid in self-management of prediabetes. There is a critical need for innovative, practical strategies to improve monitoring and management of glycemic health. In this study, using a dataset of 25,000 simultaneous interstitial glucose and noninvasive wearable smartwatch measurements, the goal is to demonstrate the feasibility of using noninvasive smartwatches and food logs recorded over 10 days, to continuously detect personalized glucose deviations and to predict the exact interstitial glucose value in real time.\n",
    "\n",
    "### Capstone 2 Project scope:\n",
    "- My goal is to re-create and improve on the model from the Nature Publication. \n",
    "- The authors were able to predict interstitial glucose (mg/dL) with a 21.2 RMSE and 14.3 % MAPE. \n",
    "- From the initial 8 features the authors engineered a total of 69 features applied to their final model. \n",
    "\n",
    "### Data Available:\n",
    "\n",
    "For each patient there is a set of files with their specific data. Note that the sampling periods are different for each, and that Dexcom is our target variable to predict.\n",
    "<br>\n",
    "\n",
    "| CSV              | Description                                                    | Source                                         | Median Sampling Period |\n",
    "|------------------|----------------------------------------------------------------|------------------------------------------------|------------------------|\n",
    "| **ACC_001**      | Tri-axial accelerometry (X-Y-Z)                                | Empatica E4 wrist-worn device                  | 0.03125 seconds        |\n",
    "| **BVP_001**      | Blood volume pulse                                             | Empatica E4 wrist-worn device                  | 0.015625 seconds       |\n",
    "| **Dexcom_001**   | Interstitial glucose concentration (mg/dL)                     | Dexcom G6, a continuous glucose monitor system | 300.0 seconds          |\n",
    "| **EDA_001**      | Electrodermal activity                                         | Empatica E4 wrist-worn device                  | 0.25 seconds           |\n",
    "| **HR_001_**      | Heart Rate                                                     | Empatica E4 wrist-worn device                  | 1.24 seconds           |\n",
    "| **IBI_001**      | Interbeat interval                                             | Empatica E4 wrist-worn device                  | 0.98442 seconds        |\n",
    "| **TEMP_001**     | Skin Temperature                                               | Empatica E4 wrist-worn device                  | 0.25 seconds           |\n",
    "| **food_log**     | Log of food intake with timestamps and nutritional information | User input                                     | As needed              |\n",
    "| **demographics_csv** | Sex, HbA1c, Patient ID                                         | User input                                     | One time               |\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### References: \n",
    "- The dataset is publicly available from PhysioNet. [Dataset](https://physionet.org/content/big-ideas-glycemic-wearable/1.1.2/001/#files-panel)\n",
    "- And the Nature publication is also published without a paywall: [Nature publication](https://www.nature.com/articles/s41746-021-00465-w#article-info)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c280ae035c25dfde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation",
   "id": "99a50f974a0cc991"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import necessary libraries",
   "id": "b89e69ef1750c0fa"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-15T22:39:38.576020Z",
     "start_time": "2024-07-15T22:39:38.565804Z"
    }
   },
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from pandas import to_datetime\n",
    "import numpy as np\n",
    "\n",
    "import wearablecompute as wc\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Patient_001 Example - Loading wristband + CGM data frames",
   "id": "997ba12fb2731cb4"
  },
  {
   "cell_type": "code",
   "id": "d4ac17bc3df9d519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:13.190184Z",
     "start_time": "2024-07-15T22:39:38.909446Z"
    }
   },
   "source": [
    "# Filepaths in the local directory\n",
    "filepaths = ['Data/001/ACC_001.csv', 'Data/001/BVP_001.csv', 'Data/001/Dexcom_001.csv', 'Data/001/EDA_001.csv', 'Data/001/HR_001.csv', 'Data/001/IBI_001.csv', 'Data/001/TEMP_001.csv']\n",
    "\n",
    "# Dictionary to store the dataframes\n",
    "dfs = {}\n",
    "\n",
    "for csv in filepaths:\n",
    "    key = csv.split('/')[-1].split('.')[0]  # Get the filename without the extension\n",
    "    dfs[key] = pd.read_csv(csv)  # Read the csv file and store the DataFrame in the dictionary"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect DFs and column names",
   "id": "8e1f47f3055b8932"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:13.198287Z",
     "start_time": "2024-07-15T22:40:13.192534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "  \n",
    "# Display the DFs and respective columns within the dictionary\n",
    "for key, df in dfs.items():\n",
    "    print(f\"DataFrame: {key}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print()"
   ],
   "id": "e162fb2361ed7367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: ACC_001\n",
      "Columns: ['datetime', ' acc_x', ' acc_y', ' acc_z']\n",
      "\n",
      "DataFrame: BVP_001\n",
      "Columns: ['datetime', ' bvp']\n",
      "\n",
      "DataFrame: Dexcom_001\n",
      "Columns: ['Index', 'Timestamp (YYYY-MM-DDThh:mm:ss)', 'Event Type', 'Event Subtype', 'Patient Info', 'Device Info', 'Source Device ID', 'Glucose Value (mg/dL)', 'Insulin Value (u)', 'Carb Value (grams)', 'Duration (hh:mm:ss)', 'Glucose Rate of Change (mg/dL/min)', 'Transmitter Time (Long Integer)']\n",
      "\n",
      "DataFrame: EDA_001\n",
      "Columns: ['datetime', ' eda']\n",
      "\n",
      "DataFrame: HR_001\n",
      "Columns: ['datetime', ' hr']\n",
      "\n",
      "DataFrame: IBI_001\n",
      "Columns: ['datetime', ' ibi']\n",
      "\n",
      "DataFrame: TEMP_001\n",
      "Columns: ['datetime', ' temp']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b1e135f901eb2604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Column renaming for consistency, strip unnecessary characters, convert to datetime",
   "id": "4968481a5a1db6b1"
  },
  {
   "cell_type": "code",
   "id": "1937fd66140359f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:25.787268Z",
     "start_time": "2024-07-15T22:40:13.199398Z"
    }
   },
   "source": [
    "# Remove leading/trailing spaces from column names\n",
    "for key, df in dfs.items():\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "# Rename the column 'Timestamp ...' in Dexcom_001 to 'datetime'\n",
    "dfs['Dexcom_001'] = dfs['Dexcom_001'].rename(columns={'Timestamp (YYYY-MM-DDThh:mm:ss)': 'datetime'})\n",
    "\n",
    "# Convert 'datetime' columns in to datetime\n",
    "for key in dfs.keys():\n",
    "    if 'datetime' in dfs[key].columns:\n",
    "        dfs[key]['datetime'] = pd.to_datetime(dfs[key]['datetime'])\n",
    "        \n",
    "# Display the DFs and respective columns within the dictionary\n",
    "for key, df in dfs.items():\n",
    "    print(f\"DataFrame: {key}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/5t8x6ck57719dt2pg7h0_xl40000gn/T/ipykernel_2276/2727465161.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dfs[key]['datetime'] = pd.to_datetime(dfs[key]['datetime'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: ACC_001\n",
      "Columns: ['datetime', 'acc_x', 'acc_y', 'acc_z']\n",
      "\n",
      "DataFrame: BVP_001\n",
      "Columns: ['datetime', 'bvp']\n",
      "\n",
      "DataFrame: Dexcom_001\n",
      "Columns: ['Index', 'datetime', 'Event Type', 'Event Subtype', 'Patient Info', 'Device Info', 'Source Device ID', 'Glucose Value (mg/dL)', 'Insulin Value (u)', 'Carb Value (grams)', 'Duration (hh:mm:ss)', 'Glucose Rate of Change (mg/dL/min)', 'Transmitter Time (Long Integer)']\n",
      "\n",
      "DataFrame: EDA_001\n",
      "Columns: ['datetime', 'eda']\n",
      "\n",
      "DataFrame: HR_001\n",
      "Columns: ['datetime', 'hr']\n",
      "\n",
      "DataFrame: IBI_001\n",
      "Columns: ['datetime', 'ibi']\n",
      "\n",
      "DataFrame: TEMP_001\n",
      "Columns: ['datetime', 'temp']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "id": "59f65a78d120542c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:25.794953Z",
     "start_time": "2024-07-15T22:40:25.789270Z"
    }
   },
   "source": [
    "#Removing all columns in Dexcom_001 except for Glucose Value and datetime.\n",
    "dfs['Dexcom_001'] = dfs['Dexcom_001'][['datetime', 'Glucose Value (mg/dL)']]\n",
    "\n",
    "dfs['Dexcom_001'] = dfs['Dexcom_001'].rename(columns={\"Glucose Value (mg/dL)\": \"glucose\"})\n",
    "\n",
    "# Remove rows where 'datetime' is null/NaN (the first twelve rows)\n",
    "dfs['Dexcom_001'].dropna(subset=['datetime'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "id": "d33f6ded86f8d95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:25.798246Z",
     "start_time": "2024-07-15T22:40:25.795476Z"
    }
   },
   "source": [
    "# Set 'datetime' column as index\n",
    "for df in dfs.values():\n",
    "    df.set_index('datetime', inplace=True)"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "fe5977d7de439ff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:26.104240Z",
     "start_time": "2024-07-15T22:40:25.798745Z"
    }
   },
   "source": [
    "for key, df in dfs.items():\n",
    "    dfs[key] = df.sort_index()\n"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "id": "78be1011f3a79cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:26.107075Z",
     "start_time": "2024-07-15T22:40:26.104899Z"
    }
   },
   "source": [
    "for key, df in dfs.items():\n",
    "    if df.index.is_monotonic_increasing or df.index.is_monotonic_decreasing:\n",
    "        print(f\"Index for DataFrame: {key} is monotonic\")\n",
    "    else:\n",
    "        print(f\"Index for DataFrame: {key} is NOT monotonic\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for DataFrame: ACC_001 is monotonic\n",
      "Index for DataFrame: BVP_001 is monotonic\n",
      "Index for DataFrame: Dexcom_001 is monotonic\n",
      "Index for DataFrame: EDA_001 is monotonic\n",
      "Index for DataFrame: HR_001 is monotonic\n",
      "Index for DataFrame: IBI_001 is monotonic\n",
      "Index for DataFrame: TEMP_001 is monotonic\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data sampling mismatch",
   "id": "174a4963dd318256"
  },
  {
   "cell_type": "code",
   "id": "f67b2ea7a269983b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:27.395661Z",
     "start_time": "2024-07-15T22:40:26.107653Z"
    }
   },
   "source": [
    "for key, df in dfs.items():\n",
    "    # Calculate time delta series\n",
    "    timedelta_series = df.index.to_series().diff()\n",
    "\n",
    "    # Compute mean/median of timedelta_series in seconds\n",
    "    mean_sampling_period_seconds = timedelta_series.mean().total_seconds()\n",
    "    median_sampling_period_seconds = timedelta_series.median().total_seconds()\n",
    "\n",
    "    # Count the number of samples\n",
    "    num_samples = len(df)\n",
    "\n",
    "    # Get the start time and end time\n",
    "    start_time = df.index.min()\n",
    "    end_time = df.index.max()\n",
    "\n",
    "    print(f\"For DataFrame '{key}', mean sampling period is {mean_sampling_period_seconds} seconds, \\\n",
    "          median sampling period is {median_sampling_period_seconds} seconds,\\\n",
    "          there are {num_samples} samples,\\\n",
    "          start time is {start_time}, and end time is {end_time}.\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For DataFrame 'ACC_001', mean sampling period is 0.038747 seconds,           median sampling period is 0.03125 seconds,          there are 20296428 samples,          start time is 2020-02-13 15:28:50, and end time is 2020-02-22 17:56:03.843750.\n",
      "\n",
      "For DataFrame 'BVP_001', mean sampling period is 0.019373 seconds,           median sampling period is 0.015625 seconds,          there are 40592838 samples,          start time is 2020-02-13 15:28:50, and end time is 2020-02-22 17:56:03.781250.\n",
      "\n",
      "For DataFrame 'Dexcom_001', mean sampling period is 304.449609 seconds,           median sampling period is 300.0 seconds,          there are 2561 samples,          start time is 2020-02-13 17:23:32, and end time is 2020-02-22 17:53:23.\n",
      "\n",
      "For DataFrame 'EDA_001', mean sampling period is 0.30998 seconds,           median sampling period is 0.25 seconds,          there are 2537046 samples,          start time is 2020-02-13 15:28:50, and end time is 2020-02-22 17:56:03.250000.\n",
      "\n",
      "For DataFrame 'HR_001', mean sampling period is 1.240044 seconds,           median sampling period is 0.0 seconds,          there are 634188 samples,          start time is 2020-02-13 15:29:00, and end time is 2020-02-22 17:56:00.\n",
      "\n",
      "For DataFrame 'IBI_001', mean sampling period is 2.950438 seconds,           median sampling period is 0.98442 seconds,          there are 266366 samples,          start time is 2020-02-13 15:33:22.059328, and end time is 2020-02-22 17:51:35.691598.\n",
      "\n",
      "For DataFrame 'TEMP_001', mean sampling period is 0.30998 seconds,           median sampling period is 0.25 seconds,          there are 2537040 samples,          start time is 2020-02-13 15:28:50, and end time is 2020-02-22 17:56:03.750000.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "id": "bb046ed3a4995030",
   "metadata": {},
   "source": [
    "\n",
    "| Dataframe   | Mean Sampling Period | Median Sampling Period | Number of Samples | Start Time                | End Time                  |\n",
    "|-------------|----------------------|------------------------|-------------------|---------------------------|---------------------------|\n",
    "| ACC_001     | 0.038747 seconds     | 0.03125 seconds        | 20296428          | 2020-02-13 15:28:50       | 2020-02-22 17:56:03.843750|\n",
    "| BVP_001     | 0.019373 seconds     | 0.015625 seconds       | 40592838          | 2020-02-13 15:28:50       | 2020-02-22 17:56:03.781250|\n",
    "| Dexcom_001  | 304.449609 seconds   | 300.0 seconds          | 2561              | 2020-02-13 17:23:32       | 2020-02-22 17:53:23       |\n",
    "| EDA_001     | 0.30998 seconds      | 0.25 seconds           | 2537046           | 2020-02-13 15:28:50       | 2020-02-22 17:56:03.250000|\n",
    "| HR_001_new  | 1.240044 seconds     | 0.0 seconds            | 634188            | 2020-02-13 15:29:00       | 2020-02-22 17:56:00       |\n",
    "| IBI_001     | 2.950438 seconds     | 0.98442 seconds        | 266366            | 2020-02-13 15:33:22.059328| 2020-02-22 17:51:35.691598|\n",
    "| TEMP_001    | 0.30998 seconds      | 0.25 seconds           | 2537040           | 2020-02-13 15:28:50       | 2020-02-22 17:56:03.750000|\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Early feature engineering on respective sampling periods",
   "id": "a9c0d650ac1b66ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Acceleration magnitude",
   "id": "27c820127407e8c4"
  },
  {
   "cell_type": "code",
   "id": "a064524a6700d3f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:28.585930Z",
     "start_time": "2024-07-15T22:40:27.396248Z"
    }
   },
   "source": [
    "# Create 'acc' column in 'ACC_001' DataFrame\n",
    "dfs['ACC_001']['acc'] = dfs['ACC_001'][['acc_x', 'acc_y', 'acc_z']].sum(axis=1).abs()\n"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "id": "ce3a785d7a39c703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:28.590085Z",
     "start_time": "2024-07-15T22:40:28.588008Z"
    }
   },
   "source": [
    "  \n",
    "# Display the DFs and respective columns within the dictionary\n",
    "for key, df in dfs.items():\n",
    "    print(f\"DataFrame: {key}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: ACC_001\n",
      "Columns: ['acc_x', 'acc_y', 'acc_z', 'acc']\n",
      "\n",
      "DataFrame: BVP_001\n",
      "Columns: ['bvp']\n",
      "\n",
      "DataFrame: Dexcom_001\n",
      "Columns: ['glucose']\n",
      "\n",
      "DataFrame: EDA_001\n",
      "Columns: ['eda']\n",
      "\n",
      "DataFrame: HR_001\n",
      "Columns: ['hr']\n",
      "\n",
      "DataFrame: IBI_001\n",
      "Columns: ['ibi']\n",
      "\n",
      "DataFrame: TEMP_001\n",
      "Columns: ['temp']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "id": "266d4de45978cee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:28.823398Z",
     "start_time": "2024-07-15T22:40:28.590556Z"
    }
   },
   "source": [
    "for key, df in dfs.items():\n",
    "    dfs[key].info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20296428 entries, 2020-02-13 15:28:50 to 2020-02-22 17:56:03.843750\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   acc_x   float64\n",
      " 1   acc_y   float64\n",
      " 2   acc_z   float64\n",
      " 3   acc     float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 774.2 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 40592838 entries, 2020-02-13 15:28:50 to 2020-02-22 17:56:03.781250\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   bvp     float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 619.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2561 entries, 2020-02-13 17:23:32 to 2020-02-22 17:53:23\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   glucose  2561 non-null   float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 40.0 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2537046 entries, 2020-02-13 15:28:50 to 2020-02-22 17:56:03.250000\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   eda     float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 38.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 634188 entries, 2020-02-13 15:29:00 to 2020-02-22 17:56:00\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   hr      634188 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 9.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 266366 entries, 2020-02-13 15:33:22.059328 to 2020-02-22 17:51:35.691598\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   ibi     266366 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 4.1 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2537040 entries, 2020-02-13 15:28:50 to 2020-02-22 17:56:03.750000\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   temp    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 38.7 MB\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confirm datetime index is monotonic",
   "id": "bb9fbfba2449a4fa"
  },
  {
   "cell_type": "code",
   "id": "bfb4cee37040eb89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T22:40:28.826184Z",
     "start_time": "2024-07-15T22:40:28.824015Z"
    }
   },
   "source": [
    "for key, df in dfs.items():\n",
    "    if df.index.is_monotonic_increasing or df.index.is_monotonic_decreasing:\n",
    "        print(f\"Index for DataFrame: {key} is monotonic\")\n",
    "    else:\n",
    "        print(f\"Index for DataFrame: {key} is NOT monotonic\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for DataFrame: ACC_001 is monotonic\n",
      "Index for DataFrame: BVP_001 is monotonic\n",
      "Index for DataFrame: Dexcom_001 is monotonic\n",
      "Index for DataFrame: EDA_001 is monotonic\n",
      "Index for DataFrame: HR_001 is monotonic\n",
      "Index for DataFrame: IBI_001 is monotonic\n",
      "Index for DataFrame: TEMP_001 is monotonic\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Early Feature Engineering",
   "id": "a0e723b8ded8c53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T23:05:34.915037Z",
     "start_time": "2024-07-15T23:05:03.216658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters for peak detection\n",
    "height = 0\n",
    "distance = 4\n",
    "prominence = 0.3\n",
    "\n",
    "# This function finds peaks in a timeseries and returns the number of peaks.\n",
    "def count_peaks(x):\n",
    "    peaks, _ = find_peaks(x, height=height, distance=distance, prominence=prominence)\n",
    "    return len(peaks)\n",
    "\n",
    "def process_df(df, resample_period='5min', calculate_peaks=False, rolling_2h=False):\n",
    "    # Resample the data\n",
    "    resampled = df.resample(resample_period)\n",
    "\n",
    "    # Initialize an empty dataframe to store results\n",
    "    df_result = pd.DataFrame(index=resampled.indices.keys())\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        # Calculate statistics for each column\n",
    "        df_result[f'{col_name}_mean'] = resampled[col_name].mean()\n",
    "        df_result[f'{col_name}_std'] = resampled[col_name].std()\n",
    "        df_result[f'{col_name}_min'] = resampled[col_name].min()\n",
    "        df_result[f'{col_name}_max'] = resampled[col_name].max()\n",
    "        df_result[f'{col_name}_q1'] = resampled[col_name].quantile(0.25)\n",
    "        df_result[f'{col_name}_q3'] = resampled[col_name].quantile(0.75)\n",
    "        df_result[f'{col_name}_skew'] = resampled[col_name].skew()\n",
    "\n",
    "        if calculate_peaks:\n",
    "            df_result[f'{col_name}_peaks'] = resampled[col_name].apply(count_peaks)\n",
    "\n",
    "        if rolling_2h:\n",
    "            rolling_2h_agg_func = df[col_name].rolling('2h').agg(['mean', 'max'])\n",
    "            df_result[f'{col_name}_2hr_mean'] = rolling_2h_agg_func['mean'].resample(resample_period).last()\n",
    "            df_result[f'{col_name}_2hr_max'] = rolling_2h_agg_func['max'].resample(resample_period).last()\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# Then apply this function to each of your dataframes\n",
    "dfs['EDA_001'] = process_df(dfs['EDA_001'], calculate_peaks=True)\n",
    "\n",
    "# Apply function for dfs that need 2h rolling features\n",
    "dfs['ACC_001'] = process_df(dfs['ACC_001'], rolling_2h=True)\n",
    "\n",
    "# Apply function for remaining dfs\n",
    "for name in ['HR_001', 'TEMP_001', 'IBI_001', 'BVP_001']:\n",
    "    dfs[name] = process_df(dfs[name])\n"
   ],
   "id": "e298768a73392754",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T23:06:28.461062Z",
     "start_time": "2024-07-15T23:06:28.452751Z"
    }
   },
   "cell_type": "code",
   "source": "print(dfs['IBI_001'].head())",
   "id": "e564e07759045208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ibi_mean   ibi_std   ibi_min   ibi_max    ibi_q1  \\\n",
      "2020-02-13 15:30:00  0.903166  0.059910  0.828163  0.984420  0.875040   \n",
      "2020-02-13 15:35:00  0.849333  0.228782  0.468771  1.140677  0.625028   \n",
      "2020-02-13 15:40:00  0.930846  0.159200  0.437520  1.078174  0.910197   \n",
      "2020-02-13 15:45:00  0.953820  0.157979  0.562526  1.250057  0.890666   \n",
      "2020-02-13 15:50:00  0.937543  0.098188  0.734409  1.125051  0.859414   \n",
      "\n",
      "                       ibi_q3  ibi_skew  \n",
      "2020-02-13 15:30:00  0.937543  0.253720  \n",
      "2020-02-13 15:35:00  1.039110 -0.492573  \n",
      "2020-02-13 15:40:00  1.023484 -2.479890  \n",
      "2020-02-13 15:45:00  1.046923 -0.681244  \n",
      "2020-02-13 15:50:00  1.000046 -0.440397  \n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "id": "b35f87f1a5451647",
   "metadata": {},
   "source": [
    "for key, df in dfs.items():\n",
    "    dfs[key].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge seperate dfs into 1 resampling on Dexcom (glucose)",
   "id": "2dffd8f2d3771e50"
  },
  {
   "cell_type": "code",
   "id": "f5d320856ceeaca3",
   "metadata": {},
   "source": [
    "# Starting with Dexcom_001 DataFrame and setting 'glucose' as the only column\n",
    "merged_df = dfs['Dexcom_001'][['glucose']]\n",
    "\n",
    "# Making sure the index is sorted\n",
    "merged_df = merged_df.sort_index()\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    # skip if the current dataframe is 'Dexcom_001'\n",
    "    if key == 'Dexcom_001':\n",
    "        continue\n",
    "\n",
    "    # make sure the df is sorted by index\n",
    "    df_sorted = df.sort_index()\n",
    "\n",
    "    # Merge with the current dataframe\n",
    "    merged_df = pd.merge_asof(merged_df, df_sorted, left_index=True, right_index=True, direction='nearest',\n",
    "                              tolerance=pd.Timedelta('4min'))\n",
    "\n",
    "merged_df.info()  # print the info of the merged dataframe\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Drop NaN values",
   "id": "52f1a80812e3989d"
  },
  {
   "cell_type": "code",
   "id": "63b1f6a0fe174083",
   "metadata": {},
   "source": [
    "# Drop rows with NaN values\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# Print the info of the DataFrame after dropping NaN rows\n",
    "merged_df.info()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect merged_df",
   "id": "b7002fd6a591e1e8"
  },
  {
   "cell_type": "code",
   "id": "a3f87c36cd01c795",
   "metadata": {},
   "source": "merged_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "677989a3dceecfa9",
   "metadata": {},
   "source": "merged_df.info()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b723762a552c359e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### All patient merge code",
   "id": "d4d26ae2420bbebb"
  },
  {
   "cell_type": "code",
   "id": "b3a2894834c42478",
   "metadata": {},
   "source": [
    "# This function finds peaks in a timeseries and returns the number of peaks.\n",
    "def count_peaks(x):\n",
    "    height = 0\n",
    "    distance = 4\n",
    "    prominence = 0.3\n",
    "    peaks, _ = find_peaks(x, height=height, distance=distance, prominence=prominence)\n",
    "    return len(peaks)\n",
    "\n",
    "\n",
    "def process_df(df, resample_period='5min', calculate_peaks=False, rolling_2h=False):\n",
    "    # Resample the data\n",
    "    resampled = df.resample(resample_period)\n",
    "\n",
    "    # Initialize an empty dataframe to store results\n",
    "    df_result = pd.DataFrame(index=resampled.indices.keys())\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        # Calculate statistics for each column\n",
    "        df_result[f'{col_name}_mean'] = resampled[col_name].mean()\n",
    "        df_result[f'{col_name}_std'] = resampled[col_name].std()\n",
    "        df_result[f'{col_name}_min'] = resampled[col_name].min()\n",
    "        df_result[f'{col_name}_max'] = resampled[col_name].max()\n",
    "        df_result[f'{col_name}_q1'] = resampled[col_name].quantile(0.25)\n",
    "        df_result[f'{col_name}_q3'] = resampled[col_name].quantile(0.75)\n",
    "        df_result[f'{col_name}_skew'] = resampled[col_name].skew()\n",
    "\n",
    "        if calculate_peaks:\n",
    "            df_result[f'{col_name}_peaks'] = resampled[col_name].apply(count_peaks)\n",
    "\n",
    "        if rolling_2h:\n",
    "            rolling_2h_agg_func = df[col_name].rolling('2h').agg(['mean', 'max'])\n",
    "            df_result[f'{col_name}_2hr_mean'] = rolling_2h_agg_func['mean'].resample(resample_period).last()\n",
    "            df_result[f'{col_name}_2hr_max'] = rolling_2h_agg_func['max'].resample(resample_period).last()\n",
    "\n",
    "    return df_result\n",
    "\n",
    "def process_patient_data(patient_id):\n",
    "    # Filepaths in the local directory\n",
    "    filepaths = [f'Data/{patient_id}/ACC_{patient_id}.csv', f'Data/{patient_id}/BVP_{patient_id}.csv',\n",
    "                 f'Data/{patient_id}/Dexcom_{patient_id}.csv', f'Data/{patient_id}/EDA_{patient_id}.csv',\n",
    "                 f'Data/{patient_id}/HR_{patient_id}.csv', f'Data/{patient_id}/IBI_{patient_id}.csv',\n",
    "                 f'Data/{patient_id}/TEMP_{patient_id}.csv']\n",
    "\n",
    "    # Dictionary to store the dataframes\n",
    "    dfs = {}\n",
    "\n",
    "    for csv in filepaths:\n",
    "        key = csv.split('/')[-1].split('.')[0]  # Get the filename without the extension\n",
    "        dfs[key] = pd.read_csv(csv)  # Read the csv file and store the DataFrame in the dictionary\n",
    "\n",
    "        # Remove leading/trailing spaces from column names\n",
    "        dfs[key].columns = dfs[key].columns.str.strip()\n",
    "\n",
    "        # Special preprocessing for Dexcom files\n",
    "        if 'Dexcom' in key:\n",
    "            dfs[key] = dfs[key].rename(columns={'Timestamp (YYYY-MM-DDThh:mm:ss)': 'datetime'})\n",
    "            dfs[key]['datetime'] = pd.to_datetime(dfs[key]['datetime'], format='mixed')\n",
    "            dfs[key] = dfs[key][['datetime', 'Glucose Value (mg/dL)']].rename(columns={\"Glucose Value (mg/dL)\": \"glucose\"})\n",
    "            dfs[key].dropna(subset=['datetime'], inplace=True)\n",
    "\n",
    "        if 'datetime' in dfs[key].columns:\n",
    "            dfs[key]['datetime'] = pd.to_datetime(dfs[key]['datetime'], format='mixed')\n",
    "\n",
    "        # Set 'datetime' column as index and sort the DataFrame by index\n",
    "        if 'datetime' in dfs[key].columns:\n",
    "            dfs[key].set_index('datetime', inplace=True)\n",
    "            dfs[key].sort_index(inplace=True)\n",
    "        \n",
    "        # Special preprocessing for ACC files\n",
    "        if 'ACC' in key:\n",
    "            dfs[key]['acc'] = dfs[key][['acc_x', 'acc_y', 'acc_z']].sum(axis=1).abs()\n",
    "\n",
    "    processed_dfs = {}\n",
    "\n",
    "    for name in [f'{prefix}_{patient_id}' for prefix in ['EDA', 'ACC', 'HR', 'TEMP', 'IBI', 'BVP']]:\n",
    "        if name.startswith('EDA'):\n",
    "            processed_dfs[name] = process_df(dfs[name], calculate_peaks=True)\n",
    "        elif name.startswith('ACC'):\n",
    "            processed_dfs[name] = process_df(dfs[name], rolling_2h=True)\n",
    "        else:\n",
    "            processed_dfs[name] = process_df(dfs[name])\n",
    "\n",
    "    # Merge all dataframes\n",
    "    merged_df = dfs[f'Dexcom_{patient_id}'][['glucose']].sort_index()\n",
    "    for key, df in processed_dfs.items():\n",
    "        if key == f'Dexcom_{patient_id}':\n",
    "            continue\n",
    "        df_sorted = df.sort_index()\n",
    "        merged_df = pd.merge_asof(merged_df, df_sorted, left_index=True, right_index=True, direction='nearest',\n",
    "                                  tolerance=pd.Timedelta('4min'))\n",
    "\n",
    "    merged_df = merged_df.dropna()\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# List of patient ids\n",
    "patient_ids = [f\"{i + 1:03d}\" for i in range(16)]  # This will generate a list: ['001', '002', ..., '015', '016']\n",
    "\n",
    "# List to store each processed dataframe\n",
    "dfs_list = []\n",
    "\n",
    "for pid in patient_ids:\n",
    "    df = process_patient_data(pid)\n",
    "\n",
    "    # Add a 'patient_id' column\n",
    "    df['patient_id'] = pid\n",
    "\n",
    "    dfs_list.append(df)\n",
    "\n",
    "    print(f'Patient {pid} is finished')\n",
    "\n",
    "# Concatenate all dataframes in the list into one\n",
    "final_df = pd.concat(dfs_list)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95c348874ab3406a",
   "metadata": {},
   "source": [
    "final_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1150ca71b0a5ce6e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "416ef1d10d3ad9a5",
   "metadata": {},
   "source": [
    "final_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3655b2d59556ff2f",
   "metadata": {},
   "source": [
    "final_df.groupby('patient_id').size()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "541d3271e8c08129",
   "metadata": {},
   "source": [
    "final_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9c720728874a55f",
   "metadata": {},
   "source": [
    "final_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4be34a3c28e953f",
   "metadata": {},
   "source": [
    "final_df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e4e5ff3554a829a",
   "metadata": {},
   "source": [
    "final_df['patient_id'] = final_df['patient_id'].astype(int)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Demographics CSV",
   "id": "f42c76c5c747d16"
  },
  {
   "cell_type": "code",
   "id": "6780721b6de7dd1",
   "metadata": {},
   "source": [
    "# Load the CSV file from the 'Data' folder in your current directory\n",
    "demographics_df = pd.read_csv('./Data/Demographics.csv')\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "demographics_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d476837b6cd7076",
   "metadata": {},
   "source": [
    "# Convert the 'datetime' index into a column\n",
    "final_df.reset_index(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Demographics Merge",
   "id": "74481c0ff3c75470"
  },
  {
   "cell_type": "code",
   "id": "cb59a1bab003cfdc",
   "metadata": {},
   "source": [
    "# Merge the patient_df with the demographics_df\n",
    "patient_df = final_df.merge(demographics_df, left_on='patient_id', right_on='ID', how='left')\n",
    "\n",
    "# Drop the 'ID' column as it's duplicate of 'patient_id' column\n",
    "patient_df = patient_df.drop('ID', axis=1)\n",
    "\n",
    "patient_df = patient_df.drop('index', axis=1)\n",
    "patient_df = patient_df.drop('level_0', axis=1)\n",
    "# Print the first 5 rows of the new DataFrame\n",
    "patient_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e394237c7af87253",
   "metadata": {},
   "source": [
    "patient_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reorder columns",
   "id": "53e6a8397b4f04fc"
  },
  {
   "cell_type": "code",
   "id": "be012405f08f768f",
   "metadata": {},
   "source": [
    "patient_df = patient_df[\n",
    "    ['datetime', 'patient_id', 'glucose', 'Gender', 'HbA1c', 'acc_mean', 'bvp_mean', 'eda_mean', 'hr_mean', 'ibi_mean',\n",
    "     'temp_mean'] + [col for col in patient_df.columns if\n",
    "                     col not in ['datetime', 'patient_id', 'glucose', 'Gender', 'HbA1c', 'acc_mean', 'bvp_mean',\n",
    "                                 'eda_mean', 'hr_mean', 'ibi_mean', 'temp_mean']]]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c13220da328fcd3",
   "metadata": {},
   "source": [
    "patient_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### patient df csv",
   "id": "2bbd9856a57e4062"
  },
  {
   "cell_type": "code",
   "id": "a5acbd109fefb0a1",
   "metadata": {},
   "source": [
    "patient_df.to_csv('patient_df.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
